{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Pytorch Hands-on\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Goals\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   If you're already a deep learning master, not the talk for you!\n-   Get up and running quickly with Deep Learning\n    -   In particular, the goal is to build neural networks you can take home today!\n-   Therefore, use *PyTorch* to get up and running quickly\n-   Outline of the session:\n    -   Basic Usage of PyTorch (Iris)\n    -   Convolutional Neural Net in PyTorch (MNIST)\n    -   GANs (using MNIST)\n-   For this lecture, I recommend using *google colaboratory*\n    -   Machine learning education and research tool setup by google, all\n        the packages are installed, just need a google account to sign in\n\n[https://colab.research.google.com](https://colab.research.google.com)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Dependencies\n\n"]},{"cell_type":"markdown","metadata":{},"source":["*However,*\n\nIf you want to follow along with a local setup:\n\nWith python and pip installed, you can pull the dependencies by pip\ninstalling (you might need to add \\`&#x2013;user\\` to the end of the command\nlines):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Optional, setup a separate virtualenv to keep everything clean\nvirtualenv ENV\nsource ENV/bin/activate\n# Download dependencies\npip install matplotlib seaborn jupyter\n# Download the CPU version of pytorch\npip install torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n# Then you could start a notebook\njupyter notebook"]},{"cell_type":"markdown","metadata":{},"source":["For a GPU-capable version of pytorch (CUDA, which means you need an\nnVidia card), check\n\n[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)\n\nNow, let's setup a new workspace\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Google Colab / Jupyter Basic Usage\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   <span class=\"underline\">[https://colab.research.google.com/notebook](https://colab.research.google.com/notebook)</span>\n-   Offers free jupyter-notebook-as-a-service in the cloud\n    -   Even offers free access to cloud-based GPUs\n-   Has all the packages we'll need for today pre-installed\n-   Demo\n    -   Basic jupyter usage\n\n    !ls # execute external commands\n\n    import os\n     # help at your fingertips\n    ?os\n\n    pi = 3.14159 # variables persists over cells\n\n    pi*2\n\n    def area(radius):\n        return pi*radius**2\n\n    area(1)\n\n    # inline plotting\n    import numpy as np\n    import matplotlib.pyplot as plt\n    \n    x = np.linspace(-3.14, 3.14, 100)\n    y = np.sin(x)\n    plt.plot(x, y)\n    plt.show()\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### PyTorch\n\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](figures/pytorch.jpeg)\n\n-   Deep learning framework based on Torch, a library for the Lua language\n-   Library written in python, user-friendly interface\n-   API is similar to numpy, can write straightforward python (cf. tensorflow)\n-   Easy to get started building networks\n-   Quickly build and train serious models, easy to experiment\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### imports\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Follow along either on the web-based service, or your own machine\n-   Lets pull in all the imports and definitions we'll need\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### :BMCOL:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["    import matplotlib\n    # matplotlib.use(\"AGG\")  # To batch graphics\n    import matplotlib.pyplot as plt\n    import os\n    import seaborn as sns\n    import numpy as np\n    import torch as th # n.b. this is somewhat non-standard\n    import torch.nn as nn\n    import torch.optim as optim\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## NN: Fisher's Irises\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Overarching Idea of (Supervised) Maching Learning\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Framework for Machine Learning: given a set of data, and set of\n    expected outputs (typically categories), build a system which learns\n    how to connect data to output\n-   Neural Network is one type, connect stacks of tensor operators with fixed linear and non-linear transformations\n-   Optimize transformation parameters so as to approximate expected outputs\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### The iris dataset and a basic network with Keras\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### :BMCOL:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](figures/iris_petal_sepal.png)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### :BMCOL:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Let's take a concrete example\n-   The iris dataset is a classic classification task, first studied by\n    Fisher in 1936.\n-   The goal is, given features measured from a particular\n    iris, classify it into one of three species\n    -   Iris setosa, virginica, versicolor.\n-   The variables are: Sepal width and length, petal width and length (all in cm).\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Iris dataset\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We begin by loading the iris dataset, helpfully available from the\nseaborn pacakge, which also lets us create plots showing the\ncorrelations between the variables.\n\n    iris = sns.load_dataset(\"iris\")\n    iris.head()\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Iris Variables\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Lets view the basic variables we have. Setosa (blue) looks easily\nseparable by the petal length and width, but versicolor and virginica\nare a little tricky.\n\n    plot = sns.pairplot(iris, hue=\"species\")\n    plot.savefig('iris.png')\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### The Logistic Function and Logistic Regression\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\\centering\n\n[figures/logistic.pdf](figures/logistic.pdf)\n\n-   The logistic (or sigmoid) function is defined as $f(x) = \\frac{1}{1+e^{-x}}$\n    -   Looks like a classic \"turn-on\" curve\n-   Concentrate on the case of two classes (cat/dog or electron/photon),\n    and ask what we want from a classifier output\n    -   We need to distinguish between the two classes using the output:\n    -   If the value is 0, it represents the classifier identifying one class (cat)\n    -   If its near 1, the classifier is identifies the other class (dog)\n    -   Thus, we need to transform the input variables into 1D, then pass through the logistic function\n-   This is a simple classification technique called *logistic regression*\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Neural Networks Overview\n\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](figures/neural_net.jpeg)\n\n-   Example shown: input vector $\\vec{x}$, goes through\n    $\\vec{y}_{hidden} = W\\vec{x} + \\vec{b}$, then $\\vec{y}_{output} =\n      \\sigma(\\vec{y}_{hidden})$ (&sigma; is some non-linear turn-on curve)\n-   I.e. hidden layer combines $\\vec{x}$ by some weights, then if the\n    weighted sum passes a threshold $\\vec{b}$, we turn on the output\n    (with the $\\sigma(x) = 1/(1+e^{-x})$ to gate the ops)\n-   Need to **train** the weight matrix $W$ and the bias vector $b$ and\n    optimize a \"loss\" function that represents a distance from the target output\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Backpropagation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   The algorithm to train neural networks is called **backpropagation**\n-   Its essentially a gradient descent implemented taking the network\n    structure into account to speed up evaluation of the partials\n-   To apply gradient descent, need a function of a single variable, called the *loss*\n    -   $L(x_i|\\sigma) = \\sum_i |f(x_i | \\sigma) - y_i|^2$ for inputs $x_i$ and known output $y_i$\n-   We start with the parameters set to arbitrary values, usually picked from e.g. unit gaussian\n-   We run a forward pass through the network and calculate the loss\n-   Using the chain rule, calculate *all* the derivates backward from the loss to\n    the higher layers\n-   Propagate changes based on the gradient $\\Delta w_i = -\\eta \\frac{\\partial f}{\\partial w_i}$\n-   For more on how backpropagation works: <span class=\"underline\">[http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)</span>\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### PyTorch Networks\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In order to classify the irises, we'll build a simple network in PyTorch.\n\n-   Helper functions to easily build neural networks are provided by the\n    `torch.nn` module (usually imported as `nn`)\n-   In PyTorch, neural networks are made as classes deriving from\n    `nn.Module`\n-   You need to provide an `__init__` method setting up the parameters\n    (i.e. attach them to `self`), and a `forward(x)` method which\n    returns the network output for the input `x`\n    -   PyTorch keeps track of the parameters and the **computational\n        graph** which is needed to efficiently perform backpropagation\n-   Interface to/from the model with the PyTorch `Tensor` class\n    (essentially a numpy array which can be interfaced with PyTorch's\n    backprop and can be placed on the GPU)\n    -   In fact, `Tensor.numpy()` will give you back a regular numpy array\n\n![img](figures/nn-1a.png)\n\n    arr = np.array([1.,2.,3.])\n    print(th.Tensor(arr)) # build from a numpy array\n    print(th.Tensor([1.,2.,3.])) # or a python array\n    print(th.linspace(1., 3., 3)) # or a helper function\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Our model will be a simple NN with a single hidden layer\n-   We start by building a Sequential model and add a Dense (fully-connected) layer, with sigmoid activation\n-   Dense: standard layer, all inputs connect to all outputs: $\\hat{y} = W\\hat{x} + \\hat{b}$\n    -   `keras.layers.Dense(output_dim)`\n    -   Can also set the initalization, add an activation layer inline, add regularizers inline, etc.\n-   Activation: essentially acts as a switch for a given node, turns output on/off based on threshold\n    -   `keras.layers.Activation(` *type* `)`\n        -   Where *type* might be:\n    -   *sigmoid*: $f(x) = \\frac{1}{1 + e^{-x}}$\n    -   *tanh*: $f(x) = \\tanh{x} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n    -   *relu*: $f(x) = \\mathrm{max}(0, x)$, 'rectified linear unit'\n    -   *softplus*: $f(x) =  \\ln{(1 + e^x)}$, smooth approx. to *relu*\n    -   *softmax*: $f_k(x) = \\frac{e^{-x_k}}{\\sum_i e^{-x_i}}$ for the $k$'th output, as last layer of categorical distribution, represents a probability distribution over the outputs\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Build a model: Python code\n\n"]},{"cell_type":"markdown","metadata":{},"source":["    \n    class MyNet(nn.Module):\n        def __init__(self):\n            # setup with nn.Module's initializer\n            super(MyNet, self).__init__()\n            # Linear is a helper creating a fully-connected layer\n            # (i.e. typical feedforward neural network)\n            self.fc1 = nn.Linear(4,128) # fc=fully-connected\n            self.fc2 = nn.Linear(128,3) # 3 possible outputs\n        def forward(self, x):\n            # Typical pattern in torch code, reuse the name x for\n            # successive layers\n            x = th.sigmoid(self.fc1(x))\n            x = self.fc2(x)\n            return x\n    \n    # Build a model\n    net = MyNet()\n    net\n\n-   Simply use the regular torch functions available in\n-   Notice, we didn't specify a softmax activation in the final layer\n    -   We'll see why when we get to training\n-   To use, simply pass in an input tensor\n\n    input = th.tensor([1.,2.,3.,4.])\n    net(input)\n\n-   Note that the network allows you to pass in several datapoints at\n    once, so you can use a tensor of dimension 1 more than the\n    dimension of a single datapoint\n-   Here, we have a 1d tensor with 4 elements in the first dimension, so\n    we can pass a dimension 2 tensor in (the first dimension is the n-inputs dimension)\n\n    # Process two inputs, get back two outputs\n    input = th.tensor([[1.,2.,3.,4.],\n                       [2.,3.,4.,5.]])\n    net(input)\n\n-   To run on the GPU, we need to make sure all the parameters of the\n    network are placed on the GPU, and the inputs are placed on the GPU.\n    -   We can use the `Tensor.cuda()` funcion which returns a GPU copy of the tensor\n    -   We can also use `Tensor.to(dev)` with `dev =\n            th.device('cuda:0')` to specify a specific CUDA device, or have a\n        way to switch to cpu (`dev = th.device('cpu')`, could have an\n        if-clause choosing cuda if available or else CPU)\n-   Similarly, we can put a cuda tensor back on the CPU with `Tensor.cpu()`\n\n    net = net.cuda() # put network on GPU \n    input = input.cuda() # and the input tensor\n    net(input) # you should see the device is now displayed\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Loss function\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   To train a network, we need a function to minimize, a *loss* function\n-   There are many loss functions built-in to pytorch, some useful ones:\n    -   `BCELoss` : binary cross-entropy loss, for classifying a single\n        output for on-off/yes-no conditions, it returns -log(output) if\n        the result should be 1, -log(1-output) if the result should be 0\n        -   This derives from a likelihood anaylsis of the binomial distribution\n    -   `CrossEntropyLoss` : cross-entropy loss, for classifying into\n        multiple categories using a one-hot encoding scheme,   \n        -log(softmax<sub>output</sub>[correct position])\n    -   `MSELoss` : mean-square error loss, useful for regression and\n        similar\n-   They have the common form `loss_fn(prediction, true_values)`\n    -   `prediction` is the network output for a batch, `true_values` are\n        the corresponding truth labels\n\nLets start with an example of binary cross-entropy loss. Use when\nthere's only 2 classes to classify. Here, we also show a quicker way\nof setting up a simple network using `Sequential` (should be familiar\nif you know Keras).\n\n    # For simple networks just containing stacks of layers, one can use\n    # the Sequential module instead, need explicit layers for the\n    # activation in this case\n    aNet = nn.Sequential(nn.Linear(2,4), nn.Sigmoid(), nn.Linear(4,1), nn.Sigmoid()).cuda()\n    \n    loss_fn = th.nn.BCELoss() # the losses live in torch.nn\n    out = aNet(th.zeros(2).cuda()) # the net is on the GPU\n    loss = loss_fn(out, th.tensor([1.]).cuda()) # one datapt\n    print(loss.item(), -th.log(out).item()) # theres no magic\n\nFor more than two outputs, we use cross-entropy loss. In pytorch, the\nloss function applies the softmax itself (so we don't need it in our\nnetwork), and then applies the -log(p) function on the true values\noutput [think of it like the -log of the networks probability for it\nto be the true value].\n\n    loss_fn = th.nn.CrossEntropyLoss()\n    # the net is on the GPU, here we create 3 inputs, each filled with zeros\n    out = net(th.zeros(3,4).cuda())\n     # for CrossEntropyLoss, the true values should be the position of the real label\n    print(loss_fn(out, th.tensor([0,1,2]).cuda()))\n    # Note that the output is the *average* of the losses of all the input items\n    #   this makes it easier to process a batch of inputs and run gradient descent\n\nJust to be explicit of what CrossEntropyLoss means.\n\n-   *softmax* normalizes the output layer so it sums to 1: $f_k(x) = \\frac{e^{-y_k}}{\\sum_i e^{-y_i}}$\n-   Cross-entropy loss is the -log(p) where p is the output of the softmax for the (known) true value\n    -   We are in the fully-labelled paradigm for training\n\n    out = net(th.zeros(1,4).cuda())\n    # use item() to extract a single value from a tensor and return as a python float\n    print(\"output of the loss_fn\", loss_fn(out, th.tensor([0]).cuda()).item())\n    # we softmax over the outputs (zeroth axis is the datapoints axis, first axis is the output axis)\n    print(th.softmax(out,1))\n    # The softmax output sums to 1, hence its like a \"probability for each possible output\"\n    print(th.softmax(out,1).sum().item())\n    # then cross entropy is the -log(p) where p is the true output, here\n    # we pretend that we know the 0th category is the true category\n    print(-th.log(th.softmax(out,1)[0,0]).item())\n\nThe closer the output of the true value is to 1, the closer the\nCrossEntropyLoss is to 0. The closer it is to 0, the CrossEntropyLoss\nwill exponentially approach -infinity. Hence, minimizing the\nCrossEntropyLoss is equivalent to maximizing the softmax output\nprobability of the true value to 1, and true value softmax outputs\nclose to 0 are exponentially penalized (its worse to be close to 0\nthan it is to be a little bit further from 1).\n\nIf you want to apply the softmax inside the network yourself, you can\nuse `NLLLoss` instead of `CrossEntropyLoss`. The reason you wouldn't\nis that to get a \"prediction\" from an unknown datapoint after training\nthe network, you can just take the position with the max of the\nnetwork output, you don't need to calculate the (relatively expensive)\nsoftmax, which is just rescaling the values so they're all between 0\nand 1.\n\n`BCELoss` expects a value between 0 and 1, so you should apply\n`sigmoid` to the final layer in this case.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Training\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Now we fit to the training data.\n-   We can set the number of `epochs`, `batch_size`, and `verbose`'ity\n    -   Epochs: number of training passes through the complete dataset\n    -   Batch size: number of datapoints to consider together when\n        updating the network\n-   We pass through the input data as a numpy array (nsamples, 4)\n-   We pass the output as (nsamples, 3) where for each sample one\n    of the positions is 1, corresponding to the correct class.\n-   We transform the raw species information (which labels classes as\n    strings: \"setosa\", \"virginica\", \"versicolor\") to the expected format\n    -   Setosa = should give `(1, 0, 0)`, i.e. 1 in the 0th position\n    -   Versicolor = `(0, 1, 0)`\n    -   Virginica = `(0, 0, 1)`\n\n    variables = iris.values[:, :4]\n    species_ = iris.values[:, 4]\n    \n    # One hot encode the species target\n    smap = {'setosa' : 0, 'versicolor' : 1, 'virginica' : 2}\n    species = np.array(list(smap[s] for s in species_))\n    \n    # To show we are simply passing numpy arrays of the data\n    print(iris.iloc[0])\n    print(\"----------\")\n    print(variables[0], \":\", species_[0], \":\", species[0])\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Training Loop\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   By default, there's no automatic training functions in PyTorch, you're expected to build it yourself\n    -   This is different from, say, Keras, which gives you a 'fit' function\n-   It does give you the tools to build the loop easily though\n-   We will fit the model to a labelled dataset by creating a \"training\n    loop\"\n-   We will pass a minibatch into the network, calculate the average\n    loss, the update the network based on minimizing that minibatch loss\n    -   This is \"Stochastic Gradient Descent\", as opposed to full gradient\n        descent, which would calculate the average on the full dataset\n        before updating the network\n    -   The trade off is that SGD only approximates the loss landscape for\n        each batch, while full GD requires much more computation for each update\n    -   Also, you can argue that SGD should generalize better since its\n        harder to fit to the datapoints and easier to fit general features \n        with the loss landscape constantly changing between updates\n-   `loss.backward()` performs the backpropagation on the computational\n    graph, calculating the partial derivatives of the weights w.r.t. loss\n    -   Allows the graph to be built up in possibly several steps before\n        computing, allowing for arbitrary flexibility\n-   An optimizer keeps track of the gradients and can potentially\n    automatically set different learning rates for different parameters\n    -   E.g. look up Adam, Adagrad, or RMSprop for instance\n    -   We'll use the basic SGD optimizer here\n-   The `torch.optim` module contains several optimizers, which take in\n    the network parameters and some options, and will keep track of the\n    gradients, and apply a gradient descent (or similar) `step` to the\n    network\n    -   We use `zero_grad` to zero out the gradients: you can also run\n        several batches and then step the optimizer\n-   `n_epochs` is the number of times through the complete dataset you will train for\n    -   The more you train, the better the fit, but you need to be careful of overfitting\n\n    # By default, pytorch operates on floats, not doubles\n    X, Y = th.tensor(variables.astype('float32')), th.tensor(species)\n    # A basic stochastic gradient descent optimizer\n    n_epochs, minibatch_size = 5, 16\n    optimizer = optim.SGD(net.parameters(), lr=0.1)\n    loss_fn = th.nn.CrossEntropyLoss()\n    optimizer.zero_grad()\n    for _ in range(n_epochs):\n      for idx in range(0,len(X),minibatch_size):\n        optimizer.zero_grad()\n        prediction = net(X[idx:idx+minibatch_size])\n        loss = loss_fn(prediction, Y[idx:idx+minibatch_size])\n        loss.backward()\n        optimizer.step()\n    print(\"Finished training!\")\n\nNow lets check the output of the trained network on some example\ninputs.\n\n    for x, y in zip(th.softmax(net(X[:5]),1), Y[:5]): print(x,y.item())\n\nLooking good. Lets check the accuracy of the network\n\n    prediction=net(X)\n    _, prediction_y = th.max(prediction,1) # returns (the max value, position of the max value)\n    print(th.sum(prediction_y==Y) / float(len(Y))) # sum the number we got correct\n\n67%, the network got stuck in a local minimum this time (you'll\nprobably get a different value).\n\nLets make a confusion matrix to find out where the network is having troubles\n\n    confusion = th.zeros(3, 3)\n    for i, j in zip(net(X).max(1)[1], Y): confusion[i,j] += 1\n    plt.matshow(confusion)\n    plt.colorbar()\n\nSo its confusing 1 for 2. We could try training more, or if the\nnetwork is simply stuck in a local minima, try to retrain, possibly\nchanging some of the values.\n\nSome exercises\n\n-   Try to increase the number of nodes in the hidden layer of the\n    network, and retraining\n-   Try adding another hidden layer to the network and retraining\n    -   You'll need to add an `fc3` and you change the `fc2` output to\n        more hidden nodes\n-   The activation function we're using `sigmoid` is prone to getting\n    stuck (if all inputs are large or small, then it will output 0 or 1\n    always, and the partial will always be 0, so its impossible to\n    train). Try using an alternative activation function, like `relu`,\n    does it work better?\n-   Try changing the optimizer. For example, try using `Adam`, does it\n    help the network converge more quickly?\n    -   In this case though, there are very few datapoints&#x2026;\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## DNN: MNIST\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Okay, we're done with the warmup, lets try to build an actual deep\n    learning network, by training an image classifier\n-   Another, more recent, classic classification task\n-   Given a 28x28 image of a handwritten digit, can you train a\n    classifier to recognize the numbers from 0 to 9?\n    -   The data was collected by NIST, and Modified to fit into a\n        standard format: 28x28 with 256 greyscale levels per pixel\n-   There is an additional library `torchvision` which has the ability\n    to download the dataset into a torch dataset, which is a class that\n    wraps up the X and Y tensors we were using\n    -   It gives us PIL images, we use the transform to change it to a\n        tensor\n    -   There are other transformations you could perform on the image:\n        cropping, normalizing, etc.\n-   The dataset is split into testing and training sets\n    -   The training set should be used to train the network, the testing\n        set should only be used to test the network performance\n    -   This helps gauge how well the training is doing to generalize on\n        unseen data\n\n    import torchvision\n    train_ = torchvision.datasets.MNIST('/tmp', download=True, train=True, transform=torchvision.transforms.ToTensor())\n    test_ = torchvision.datasets.MNIST('/tmp', download=True, train=False, transform=torchvision.transforms.ToTensor())\n\nLets see what it looks like:\n\n    print(train_.data.shape) # X\n    print(train_.targets.shape) # Y\n    print(train_.data[0], train_.targets[0])\n\nSo, the data is a 60000x28x28 tensor, containing integer values from 0\nto 255, and the output is a 60000 element tensor encoding the true\nlabel in exactly the format needed for CrossEntropyLoss (and\ncorresponding to the real digit value). Lets view some images:\n\n    for i in range(8):\n      plt.subplot(2,4,i+1)\n      plt.imshow(train_.data[i], cmap='gray')\n    print(train_.targets[:8])\n\nWe can use another facility of pytorch to create a `DataLoader`, which\nwraps up our minibatching code from the previous section.\n\n    train = th.utils.data.DataLoader(train_, batch_size=64)\n    test = th.utils.data.DataLoader(test_, batch_size=64)\n\nIt makes it easier to do the training loop.\n\n    for x, y in train:\n        print(x.shape, y.shape)\n        break\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Convolutional Filter in PyTorch\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Notice that x is actually given as a 4D tensor. The axis of length 1\nis for image **channels**. If we had a color image, it would have length\n3, after we pass it through a convolutional layer, it has # channels\n== # filters in the layer.\n\nConvolutional filters in pytorch are accessed with\n`nn.Conv2d(nchannels_in, nchannels_out, filter_size)`.  Filter size\ncan be a scalar, in which case the filter is square, or you can pass a\n2-tuple to specify the width and height.\n\n    conv = nn.Conv2d(1, 5, 7)\n    conv(x).shape\n\nNo filter padding, so it has reduced the image size. The `padding`\noptional argument can be used to pad the image.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### A Convolutional Network\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Lets make a network. It consists of a few layers of convolutions of\nvarying sizes, then, we will turn the filter image into a 1d array,\nand process it through a fully-connected hidden layer. We use relu\ninstead of sigmoid here. relu(x) = 0 for negative x and relu(x) = x\nfor postive x, it tends to be better for network, since it is harder\nto saturate, and shut a node down.\n\n    class ConvNet(nn.Module):\n        def __init__(self):\n            super(ConvNet, self).__init__()\n            self.c1 = nn.Conv2d(1, 5, 7) # 28x28 -> 22x22\n            self.c2 = nn.Conv2d(5, 10, 5) # 22x22 -> 18x18\n            self.c3 = nn.Conv2d(10, 5, 3) # 18x18 -> 16x16\n            self.fc1 = nn.Linear(16*16*5,100) # 5 channels of 16x16 images\n            self.fc2 = nn.Linear(100,10)\n        def forward(self, x):\n            x = th.relu(self.c1(x))\n            x = th.relu(self.c2(x))\n            x = th.relu(self.c3(x))\n            # convert the 5x16x16 image into a 1d array with 5*16*16\n            # elements, i.e. \"flatten\" it\n            x = x.reshape(x.shape[0],-1)\n            x = th.relu(self.fc1(x))\n            x = self.fc2(x)\n            return x\n    \n    net = ConvNet()\n    net(x).shape\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### A More Complete Training Loop\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Since we have a testing sample this time, lets instrument our training loop a little\n-   We keep track of the running average of the loss / datapoint, and\n    after 300 minibatches we print out the running average loss, as well\n    as the average loss over the testing dataset\n-   We can check for overtraining (the test set will start to diverge\n    from the training set)\n    -   Here, its a bit bumpy to tell, but you can see the training loss\n        gets lower than the testing, indicating a possible saturation\n        point for this model\n-   We also only put the data on the GPU as needed\n    -   GPU memory is a limited resource, we need to keep the network\n        parameters and the data, so can't put the whole dataset on the GPU\n        this time (unless google gives you an A100 or something&#x2026;)\n\n    d = th.device('cuda')\n    net = ConvNet()\n    net.to(d)\n    \n    optim = th.optim.Adam(net.parameters(), lr=0.002)\n    lossf = th.nn.CrossEntropyLoss()\n    for epoch in range(5):\n      tl,tn = 0.,0.\n      for ii, (i, o) in enumerate(train):\n        i,o = i.to(d), o.to(d)\n        optim.zero_grad()\n        loss = lossf(net(i), o)\n        loss.backward()\n        optim.step()\n        tl+=loss.item()*i.size(0)\n        tn+=i.size(0)\n        if ii % 300 == 0:\n          print(f\"epoch {epoch} batch {ii:03d}\", end=' ')\n          print(f\"train: {tl/tn:5f}\", end=' ')\n          tl,tn = 0.,0.\n          for i, o in test:\n            i,o = i.to(d), o.to(d)\n            loss = lossf(net(i), o)\n            tl+=loss.item()*i.size(0)\n            tn+=i.size(0)\n          print(f\"test: {tl/tn:5f}\")\n          tl,tn = 0.,0.\n\nThe model starts with a loss of -2.3, and saturates at about 0.05,\nthis means that the model is, on average, giving the correct item a\nprobability of 0.10 (10 categories, so this is equivalent to\ncompletely random) at the beginning, and giving the correct item an\noutput of 0.95 at the end of training.\n\n    np.exp(-2.3), np.exp(-0.05)\n\nLets check the accuracy on the test set. In this case, we won't be\nable to compute all the predictions in one pass (we would need to fit\nthe whole test set on the GPU!).\n\n    correct, tot = 0, 0\n    for x, y in test:\n        x, y = x.to(d), y.to(d)\n        prediction = net(x)\n        _, prediction_y = th.max(prediction,1) # returns (the max value, position of the max value)\n        correct += th.sum(prediction_y==y)\n        tot += len(y)\n    print(f\"Acc.: {correct/float(tot):.3f}\")\n\nExercises:\n\n-   Compare the speed of the network on and off the GPU\n    -   Convolutional layers are the reason we need beefy GPUs for deep learning!\n-   Modify the training loop to keep track of the average losses. Plot\n    loss vs epoch for testing and training, do they keep up with each\n    other?\n-   Draw the confusion matrix for the testing set, like we did for the\n    irises. Are there particular combinations its misclassifying?\n-   Find examples the network is misclassifying and see what they look\n    like. Do you think you would have classified them correctly?\n-   Try adding more layers to the network, whats the best accuracy you can get?\n-   With too deep a network you'll find it hard to stop from\n    overtraining, try adding some regularization layers, such as\n    dropout, illustrated below, or `nn.BatchNorm2d`\n    -   Dropout helps by only allowing part of the network to see the\n        datapoint each training, so effectively you're training an\n        ensemble of networks\n    -   BatchNorm smooths out the distribution of the values passed\n        between layers as all the nodes update their weights, its\n        generally always used these days (or something similar) due to how\n        much it speeds up training large networks\n    -   With these layers you need to put the network into training or\n        evaluation mode. Use `net.train()`, and `net.eval()` respectively\n-   Torchvision also comes with classic deep learning networks, like VGG\n    and ResNet. Search the documentation and setup one of these networks\n    to solve the MNIST classification problem. You might need to change\n    the final layer of the network in order to do this.\n-   Similarly, it comes with a variety of classic datasets. Try building\n    a network to classify, for example, CIFAR10 (small color images in\n    the categories: airplane, automobile, bird, cat, deer, dog, frog,\n    horse, ship, truck)\n\n    drop = nn.Dropout(0.5)\n    print(th.ones(5))\n    print(drop(th.ones(5)))\n\nNote that the dropout compensates for the nodes it sets to zero by\nincreasing the value of the remaining non-zero nodes, so the\ndownstream node will see values of the same order of magnitude with or\nwithout the dropout.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## GAN\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The discriminator network can just be our network from before, with a\nsingle output: is the image real or fake?\n\n    train = th.utils.data.DataLoader(train_, batch_size=32)\n\n    class Discr(nn.Module):\n        def __init__(self):\n            super(Discr, self).__init__()\n            self.c1 = nn.Conv2d(1, 16, 7) # 28x28 -> 22x22\n            self.c2 = nn.Conv2d(16, 64, 5) # 22x22 -> 18x18\n            self.c3 = nn.Conv2d(64, 8, 3) # 18x18 -> 16x16\n            self.fc1 = nn.Linear(16*16*8,100) # 8 channels of 16x16 images\n            self.fc2 = nn.Linear(100,1)\n            self.drop = nn.Dropout(0.5)\n        def forward(self, x):\n            x = th.relu(self.drop(self.c1(x)))\n            x = th.relu(self.drop(self.c2(x)))\n            x = th.relu(self.drop(self.c3(x)))\n            # convert the 5x16x16 image into a 1d array with 5*16*16\n            # elements, i.e. \"flatten\" it\n            x = x.reshape(x.shape[0],-1)\n            x = th.relu(self.fc1(x))\n            x = th.sigmoid(self.fc2(x))\n            return x\n    \n    dis = Discr().cuda()\n    print(dis(th.randn(1,1,28,28).cuda()).shape)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Generator\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Here is the more interesting part. We want to take random noise, and\nthen output an image. We need to basically do the inverse of\nconvolving, that is, we take a image and pass it to the next layer\n**adding** features based on the input, rather than searching for\nthem. We can use a `ConvTranspose2d` layer to do this\n\n    class Gen(nn.Module):\n      def __init__(self, input_size=100):\n        super(Gen, self).__init__()\n        self.input_size = input_size\n        self.fc1 = nn.Linear(self.input_size, 16*16*8)\n        self.cc1 = nn.ConvTranspose2d(8,64,3) # 64x18x18\n        self.bn1 = nn.BatchNorm2d(64)\n        self.cc2 = nn.ConvTranspose2d(64,16,5) # 16x22x22\n        self.bn2 = nn.BatchNorm2d(16)\n        self.cc3 = nn.ConvTranspose2d(16,1,7) # 1x28x28\n      def forward(self, x):\n        x = th.relu(self.fc1(x))\n        x = x.view(-1,8,16,16)\n        x = th.relu(self.bn1(self.cc1(x)))\n        x = th.relu(self.bn2(self.cc2(x)))\n        x = th.sigmoid(self.cc3(x))\n        return x\n    \n    gen = Gen().cuda()\n    dis(gen(th.randn(10,100).cuda())).shape\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Training Loop\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Notice we put dropout in the discriminator (its better to have a\nsmaller capacity discriminator, since it tends to be easier to train\nthe discriminator than the generator), and BatchNorm in the\ngenerator. We want to speed up the generator training and control the\ndiscriminator training. GANs are delicate!\n\nWe need to train both the discriminator and the generator. We set up\nseparate optimizers for each.\n\nOur training loop starts by outputting sample images the generator is\ncurrently creating. We use the same latent vectors each time, so we\ncan see the evolution of the same vector.\n\nIn the loop, we update the discriminator by converging the output\ntoward 1 for real images, and toward 0 for generated images.\n\nWe then update the generator, by passing the generated images through\nthe discriminator, and trying to send the value of the discriminator\ntoward 1 (by changing the **generator** weights, the discriminator\nweights are fixed during this update).\n\nWe use `requires_grad` to turn on/off the caching of values, which can\nspeed up the training (we should/could have done this in the training\nphase of the previous classification task also).\n\nWe keep track of the average loss for the discriminator and generator\nupdates separately and record them at the end of each epoch. This is\nparticularly important here as GANs are prone to collapse: one of the\ngenerator or discriminator becomes too good, and the other no longer\ntrains. This is usually seen by one of the losses going to 0, and the\nother diverging. A good training run should have the generator and\ndiscriminator fighting with each other, and small fluctuations in each\nloss.  Another thing to look out for is **mode collapse**: this is where\nthe generator only outputs a single image, effectively ignoring the\nrandom noise input. If all the images being produced in our test step\nare the same, we probably have mode collapse.\n\n    import datetime\n    g_optim = th.optim.Adam(gen.parameters(), lr=2e-4)\n    d_optim = th.optim.Adam(dis.parameters(), lr=2e-4)\n    lossf = nn.BCELoss()\n    \n    gen.train(); dis.train()\n    # keep the input latent vectors for images the same in each epoch\n    noise = th.randn((25,gen.input_size)).cuda()\n    gloss, dloss = [], []\n    for epoch in range(25):\n      print(f\"--- Epoch {epoch} {datetime.datetime.now().strftime('%H:%M:%S')}\")\n      gen.eval()\n      im = gen(noise).view(-1,28,28)\n      plt.clf()\n      plt.gcf().set_size_inches(25, 25)\n      for i in range(25):\n        plt.subplot(5,5,i+1)\n        plt.imshow(im[i].detach().cpu(), cmap='gray')\n      plt.savefig(f'images/fc_{epoch:03d}.png')\n      plt.clf()\n      gen.train()\n      gl, gn = 0., 0.\n      dl, dn = 0., 0.\n      for ii, (i, o) in enumerate(train):\n        i, o = i.cuda(), o.cuda()\n        ones = th.tensor([1.]*i.size(0)).view(-1,1).cuda()\n        zeros = th.tensor([0.]*i.size(0)).view(-1,1).cuda()\n        randn = lambda: th.randn(i.size(0), gen.input_size).cuda()\n        # update dis, try to distinguish real (from the MNIST dataset)\n        # from fake (from gen) images\n        d_optim.zero_grad()\n        gen.requires_grad=False\n        dis.requires_grad=True\n        loss = lossf(dis(i), ones)\n        loss.backward()\n        dl += loss; dn += i.size(0)\n        loss = lossf(dis(gen(randn())), zeros)\n        loss.backward()\n        dl += loss; dn += i.size(0)\n        d_optim.step()\n    \n        # update gen, try to fool the dis network by driving its output on\n        # fake images toward 1 (by only changing the gen network)\n        g_optim.zero_grad()\n        dis.requires_grad=False\n        gen.requires_grad=True\n        loss = lossf(dis(gen(randn())), ones)\n        loss.backward()\n        gl += loss; gn += i.size(0)\n        loss = lossf(dis(gen(randn())), ones)\n        loss.backward()\n        gl += loss; gn += i.size(0)\n        g_optim.step()\n    \n      print(f\"dis: {(dl/dn).item():.3f} gen: {(gl/gn).item():.3f}\")\n      gloss.append((gl/gn).item())\n      dloss.append((dl/dn).item())\n    \n    print(\"Done\")\n\nImages we save can be displayed in colab with code like this:\n\n    from IPython.display import Image\n    Image(filename='images/fc_004.png')\n\nExample images over epochs:\n\nEpoch 0:\n![img](images/fc_000.png)\nEpoch 1:\n![img](images/fc_001.png)\nEpoch 2:\n![img](images/fc_002.png)\nEpoch 3:\n![img](images/fc_003.png)\nEpoch 5:\n![img](images/fc_005.png)\nEpoch 10:\n![img](images/fc_010.png)\nEpoch 24:\n![img](images/fc_024.png)\n\nExercises:\n\n-   The training loop is sparsely instrumented in this case, so it can\n    take a while to see whats going on. This can be a problem if the\n    training collapses (i.e. it will waste lots of time). Try moving the\n    loss output inside the batch loop, and output every few hundred\n    batches\n-   In this case, using batchnorm on the generator will greatly\n    regularize and speed up the training.  What happens if you remove\n    the normalization? [when I tried, I saw mode collapse]\n-   Try adding more or fewer layers, how does it affect the speed and\n    quality of generation?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Complete Examples\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Iris Classification with a Simple NN\n\n"]},{"cell_type":"markdown","metadata":{},"source":["    import seaborn as sns\n    import torch as th\n    import torch.nn as nn\n    import torch.optim as optim\n    \n    iris = sns.load_dataset(\"iris\")\n    \n    plot = sns.pairplot(iris, hue=\"species\")\n    plot.savefig('iris.png')\n    \n    # A simple feedforward network in pytorch\n    class MyNet(nn.Module):\n        def __init__(self):\n            super(MyNet, self).__init__()\n            self.fc1 = nn.Linear(4,128) # fc=fully-connected\n            self.fc2 = nn.Linear(128,3) # 3 possible outputs\n        def forward(self, x):\n            x = th.sigmoid(self.fc1(x))\n            x = self.fc2(x)\n            return x\n    \n    net = MyNet()\n    variables = iris.values[:, :4]\n    species_ = iris.values[:, 4]\n    \n    smap = {'setosa' : 0, 'versicolor' : 1, 'virginica' : 2}\n    species = np.array(list(smap[s] for s in species_))\n    X, Y = th.tensor(variables.astype('float32')), th.tensor(species)\n    \n    # A basic (uninstrumented) pytorch training loop\n    n_epochs, minibatch_size = 15, 16\n    optimizer = optim.SGD(net.parameters(), lr=0.01)\n    loss_fn = th.nn.CrossEntropyLoss()\n    optimizer.zero_grad()\n    for _ in range(n_epochs):\n      for idx in range(0, len(X), minibatch_size):\n        optimizer.zero_grad()\n        prediction = net(X[idx:idx+minibatch_size])\n        loss = loss_fn(prediction, Y[idx:idx+minibatch_size])\n        loss.backward()\n        optimizer.step()\n    \n    prediction=net(X)\n    _, prediction_y = th.max(prediction,1) # returns (the max value, position of the max value)\n    print(th.sum(prediction_y==Y) / float(len(Y))) # sum the number we got correct\n    \n    confusion = th.zeros(3, 3)\n    for i, j in zip(net(X).max(1)[1], Y): confusion[i,j] += 1\n    plt.matshow(confusion)\n    plt.colorbar()\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### MNIST Classification with a CNN\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Make sure you turn on the GPU in colab!\n\n    import torch as th\n    import torch.nn as nn\n    import torch.optim as optim\n    import torchvision\n    \n    train_ = torchvision.datasets.MNIST('/tmp', download=True, train=True, transform=torchvision.transforms.ToTensor())\n    test_ = torchvision.datasets.MNIST('/tmp', download=True, train=False, transform=torchvision.transforms.ToTensor())\n    train = th.utils.data.DataLoader(train_, batch_size=64)\n    test = th.utils.data.DataLoader(test_, batch_size=64)\n    \n    class ConvNet(nn.Module):\n        def __init__(self):\n            super(ConvNet, self).__init__()\n            self.c1 = nn.Conv2d(1, 5, 7) # 28x28 -> 22x22\n            self.c2 = nn.Conv2d(5, 10, 5) # 22x22 -> 18x18\n            self.c3 = nn.Conv2d(10, 5, 3) # 18x18 -> 16x16\n            self.fc1 = nn.Linear(16*16*5,100) # 5 channels of 16x16 images\n            self.fc2 = nn.Linear(100,10)\n        def forward(self, x):\n            x = th.relu(self.c1(x))\n            x = th.relu(self.c2(x))\n            x = th.relu(self.c3(x))\n            # convert the 5x16x16 image into a 1d array with 5*16*16\n            # elements, i.e. \"flatten\" it\n            x = x.reshape(x.shape[0],-1)\n            x = th.relu(self.fc1(x))\n            x = self.fc2(x)\n            return x\n    \n    d = th.device('cuda')\n    net = ConvNet()\n    net.to(d)\n    \n    # slightly more featureful training loop\n    optim = th.optim.Adam(net.parameters(), lr=0.002)\n    lossf = th.nn.CrossEntropyLoss()\n    for epoch in range(5):\n      tl,tn = 0.,0.\n      for ii, (i, o) in enumerate(train):\n        optim.zero_grad()\n        loss = lossf(net(i.to(d)), o.to(d))\n        loss.backward()\n        optim.step()\n        tl += loss.item()*i.size(0) # loss returns the *avg.* loss\n        tn += i.size(0)\n        if ii % 300 == 0:\n          print(f\"epoch {epoch} batch {ii:03d}\", end=' ')\n          print(f\"train: {tl/tn:5f}\", end=' ')\n          tl,tn = 0.,0.\n          for i, o in test:\n            loss = lossf(net(i.to(d)), o.to(d))\n            tl += loss.item()*i.size(0)\n            tn += i.size(0)\n          print(f\"test: {tl/tn:5f}\")\n          tl,tn = 0.,0.\n    \n    correct, tot = 0, 0\n    for x, y in test:\n        x, y = x.to(d), y.to(d)\n        prediction = net(x)\n        _, prediction_y = th.max(prediction,1) # returns (the max value, position of the max value)\n        correct += th.sum(prediction_y==y)\n        tot += len(y)\n    print(f\"Acc.: {correct/float(tot):.3f}\")\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### MNIST GAN\n\n"]},{"cell_type":"markdown","metadata":{},"source":["    import matplotlib.pyplot as plt\n    import torch as th\n    import torch.nn as nn\n    import torch.optim as optim\n    import torchvision\n    import datetime\n    import os\n    \n    train_ = torchvision.datasets.MNIST('/tmp', download=True, train=True, transform=torchvision.transforms.ToTensor())\n    train = th.utils.data.DataLoader(train_, batch_size=64)\n    \n    class Discr(nn.Module):\n        def __init__(self):\n            super(Discr, self).__init__()\n            self.c1 = nn.Conv2d(1, 16, 7) # 28x28 -> 22x22\n            self.c2 = nn.Conv2d(16, 64, 5) # 22x22 -> 18x18\n            self.c3 = nn.Conv2d(64, 8, 3) # 18x18 -> 16x16\n            self.fc1 = nn.Linear(16*16*8,100) # 8 channels of 16x16 images\n            self.fc2 = nn.Linear(100,1)\n            self.drop = nn.Dropout(0.5)\n        def forward(self, x):\n            x = th.relu(self.drop(self.c1(x)))\n            x = th.relu(self.drop(self.c2(x)))\n            x = th.relu(self.drop(self.c3(x)))\n            # convert the 5x16x16 image into a 1d array with 5*16*16\n            # elements, i.e. \"flatten\" it\n            x = x.reshape(x.shape[0],-1)\n            x = th.relu(self.fc1(x))\n            x = th.sigmoid(self.fc2(x))\n            return x\n    \n    class Gen(nn.Module):\n      def __init__(self, input_size=100):\n        super(Gen, self).__init__()\n        self.input_size = input_size\n        self.fc1 = nn.Linear(self.input_size, 16*16*8)\n        self.cc1 = nn.ConvTranspose2d(8,64,3) # 64x18x18\n        self.bn1 = nn.BatchNorm2d(64)\n        self.cc2 = nn.ConvTranspose2d(64,16,5) # 16x22x22\n        self.bn2 = nn.BatchNorm2d(16)\n        self.cc3 = nn.ConvTranspose2d(16,1,7) # 1x28x28\n      def forward(self, x):\n        x = th.relu(self.fc1(x))\n        x = x.view(-1,8,16,16)\n        x = th.relu(self.bn1(self.cc1(x)))\n        x = th.relu(self.bn2(self.cc2(x)))\n        x = th.sigmoid(self.cc3(x))\n        return x\n    \n    gen = Gen().cuda()\n    dis = Discr().cuda()\n    \n    \n    g_optim = th.optim.Adam(gen.parameters(), lr=2e-4)\n    d_optim = th.optim.Adam(dis.parameters(), lr=2e-4)\n    lossf = nn.BCELoss()\n    \n    gen.train(); dis.train()\n    # keep the input latent vectors for images the same in each epoch\n    noise = th.randn((25,gen.input_size)).cuda()\n    gloss, dloss = [], []\n    for epoch in range(25):\n      print(f\"--- Epoch {epoch} {datetime.datetime.now().strftime('%H:%M:%S')}\")\n      gen.eval()\n      im = gen(noise).view(-1,28,28)\n      plt.clf()\n      plt.gcf().set_size_inches(25, 25)\n      for i in range(25):\n        plt.subplot(5,5,i+1)\n        plt.imshow(im[i].detach().cpu(), cmap='gray')\n      os.system('mkdir -p images')\n      plt.savefig(f'images/fc_{epoch:03d}.png')\n      plt.clf()\n      gen.train()\n      gl, gn = 0., 0.\n      dl, dn = 0., 0.\n      for ii, (i, o) in enumerate(train):\n        i, o = i.cuda(), o.cuda()\n        ones = th.tensor([1.]*i.size(0)).view(-1,1).cuda()\n        zeros = th.tensor([0.]*i.size(0)).view(-1,1).cuda()\n        randn = lambda: th.randn(i.size(0), gen.input_size).cuda()\n        # update dis\n        d_optim.zero_grad()\n        gen.requires_grad=False\n        dis.requires_grad=True\n        loss = lossf(dis(i), ones)\n        loss.backward()\n        dl += loss; dn += i.size(0)\n        loss = lossf(dis(gen(randn())), zeros)\n        loss.backward()\n        dl += loss; dn += i.size(0)\n        d_optim.step()\n    \n        # update gen\n        g_optim.zero_grad()\n        dis.requires_grad=False\n        gen.requires_grad=True\n        loss = lossf(dis(gen(randn())), ones)\n        loss.backward()\n        gl += loss; gn += i.size(0)\n        loss = lossf(dis(gen(randn())), ones)\n        loss.backward()\n        gl += loss; gn += i.size(0)\n        g_optim.step()\n    \n      print(f\"dis: {(dl/dn).item():.3f} gen: {(gl/gn).item():.3f}\")\n      gloss.append((gl/gn).item())\n      dloss.append((dl/dn).item())\n    \n    print(\"Done\")\n\n"]}],"metadata":[["org"],null,null],"nbformat":4,"nbformat_minor":0}